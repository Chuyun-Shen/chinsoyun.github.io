<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Q-learning（1）</title>
    <url>/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/</url>
    <content><![CDATA[<h2 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h2><p>李宏毅老师主页18年机器学习教程： <a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html" target="_blank" rel="noopener">http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html</a> （老师的2020年视频也出了）</p>
 <a id="more"></a> 
<h3 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h3><p>值函数分为： State value function ， State-action value function </p>
<p>状态值函数$V_\pi(s)$，是对状态s的估计</p>
<h4 id="如何估计状态值函数"><a href="#如何估计状态值函数" class="headerlink" title="如何估计状态值函数"></a>如何估计状态值函数</h4><p><strong>monte-carlo</strong>方法：</p>
<p>对多个episode求平均，可以用神经网咯拟合</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586575571217.png" alt="1586575571217"></p>
<p><strong>TD</strong>方法：</p>
<p>两个状态之间的值函数之差是reward，拟合reward就可以了</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586575379396.png" alt="1586575379396"></p>
<p>注：MC方法相较来说<strong>方差</strong>更大，并且这两种估计方法可能结果不一样大</p>
<h3 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h3><p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586576088554.png" alt="1586576088554" style="zoom: 33%;"></p>
<p>对于一个状态，选择一个最大的Q func就可以了</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586585731110.png" alt="1586585731110" style="zoom:33%;"></p>
<p>证明：</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586585767328.png" alt="1586585767328" style="zoom:33%;"></p>
<h4 id="三个技巧"><a href="#三个技巧" class="headerlink" title="三个技巧"></a>三个技巧</h4><p><strong>Target network</strong></p>
<p>由于对q func的拟合是用的两个q之差与reward的比较，所以需要固定住一个</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586585935056.png" alt="1586585935056" style="zoom: 33%;"></p>
<p><strong>Exploration</strong></p>
<p>策略更新的方式是选择q(s,a)最大的，这在前面阶段肯定是不合适的，因为很多(s,a)没有被遍历过。</p>
<p>两种解决方法如下图</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586586846153.png" alt="1586586846153" style="zoom:33%;"></p>
<p><strong>replay buffer</strong></p>
<p>弄一个缓存，里面放（st,at,rt,st+1）</p>
<p>每次从buffur里取一个batch，然后训练q func</p>
<h4 id="Typical-Q-Learning-Algorithm"><a href="#Typical-Q-Learning-Algorithm" class="headerlink" title="Typical Q-Learning Algorithm"></a>Typical Q-Learning Algorithm</h4><p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586587733417.png" alt="1586587733417"></p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
  </entry>
  <entry>
    <title>强化基础复习</title>
    <url>/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/</url>
    <content><![CDATA[<a id="more"></a>
<p>看李宏毅老师的深度强化会感觉比较东西比较偏应用，正好要看多智体相关的东西，找了张伟楠老师的主页 <a href="http://wnzhang.net/" target="_blank" rel="noopener">http://wnzhang.net/</a> </p>
<h2 id="bellman-equation-for-value-function"><a href="#bellman-equation-for-value-function" class="headerlink" title="bellman equation for value function"></a>bellman equation for value function</h2><p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586868221816.png" alt="1586868221816"></p>
<p>$P_{s\pi}$ is state transition. At state s, use policy pi</p>
<p>Optimal value function:</p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586868369635.png" alt="1586868369635"></p>
<h2 id="值函数迭代-amp-策略迭代"><a href="#值函数迭代-amp-策略迭代" class="headerlink" title="值函数迭代&amp;策略迭代"></a>值函数迭代&amp;策略迭代</h2><p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586917042173.png" alt="1586917042173"></p>
<p>需要已知转移矩阵和reward function</p>
<p>这里的reward在这里应该是假设只依赖于状态s</p>
<p>动态规划：</p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586941139436.png" alt="1586941139436" style="zoom:33%;"></p>
<h2 id="model-free"><a href="#model-free" class="headerlink" title="model free"></a>model free</h2><h3 id="model-free-prediction"><a href="#model-free-prediction" class="headerlink" title="model-free prediction"></a>model-free prediction</h3><p><strong>MC</strong></p>
<script type="math/tex; mode=display">
G_t=R_{t+1}+\gamma R_{t+2}+...+\gamma^{T-1}R_{T}</script><script type="math/tex; mode=display">
V^{\pi}(s)=E[G_t|s_t=s,\pi]\approx \frac{1}{N}\sum_{i=1}^NG^{(i)}_{t}</script><p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586919493957.png" alt="1586919493957" style="zoom:33%;"></p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586940567481.png" alt="1586940567481" style="zoom:33%;"></p>
<p><strong>TD</strong></p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586919744733.png" alt="1586919744733" style="zoom:33%;"></p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586940746277.png" alt="1586940746277" style="zoom:33%;"></p>
<h3 id="model-free-control"><a href="#model-free-control" class="headerlink" title="model-free control"></a>model-free control</h3><p><strong>$\epsilon$-greedy</strong></p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1587038476938.png" alt="1587038476938" style="zoom:33%;"></p>
<p>前面的mc和td的目的是估计出值函数，<strong>control是为了得到最优的policy</strong></p>
<p><strong>mc control</strong></p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586954094046.png" alt="1586954094046" style="zoom:33%;"></p>
<p><strong>SARSA</strong></p>
<p>state-action-reward-state-action  </p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1587038173717.png" alt="1587038173717" style="zoom:33%;"></p>
<p>可以看到，这里是一个q的预测并不是control，但是q func得到之后就很容易得到pi，因为对于同一个s，选择最大的q(s,a)就可以了</p>
<p>在线学习版本：</p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1587039060349.png" alt="1587039060349"></p>
<p>似乎sarsa没有看到有用off-policy的，离线的可能就是q-learning了</p>
<p><strong>Q-learning</strong></p>
<p>可以参看之前的Q-learning（1）里面的q-learning，与本文的表示方式稍微有点不一样</p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1587040735589.png" alt="1587040735589" style="zoom:33%;"></p>
<p>这里的$\mu$是 交互的策略</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
  </entry>
  <entry>
    <title>强化学习PPO</title>
    <url>/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/</url>
    <content><![CDATA[<h2 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h2><h3 id="basic"><a href="#basic" class="headerlink" title="basic"></a>basic</h3><p>每一个eposide记录为$\tau$</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/4155986-0da7e5f276ec5aca.webp" style="zoom:80%;"></p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583120692790.png" alt="1583120692790" style="zoom: 33%;"></p>
 <a id="more"></a> 
<p>$p_\theta(\tau)$是按策略参数为$\theta$得到这样一个trajectory的概率</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583120878285.png" alt="1583120878285" style="zoom: 33%;"></p>
<p>要提高这个期望，使用梯度上升，对$\theta$求梯度使用上式中的中间这个求和形式，使用技巧</p>
<script type="math/tex; mode=display">
\bigtriangledown f(x) = f(x)\bigtriangledown \log{f(x)}</script><p>这样可以转化为概率采样的形式，</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\bigtriangledown\overline{R_\theta} & =\bigtriangledown\sum_{\tau}{R(\tau)p_\theta(\tau)}
\\& = \sum_\tau{R(\tau)p_\theta(\tau)\bigtriangledown \log{p_\theta(\tau)}}
\\& = E_{\tau\sim p_\theta}(R(\tau)\bigtriangledown{\log{p_\theta(\tau)}})
\\& \approx \frac{1}{N}\sum_{n=1}^N{R(\tau^n)\bigtriangledown{\log{p_\theta(\tau^n)}}}
\\& = \frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_n}R(\tau^n)\bigtriangledown{\log{p_\theta (a_t^n|s_t^n)}}
\end{aligned}</script><p>最后一行中因为我们的梯度不关心$p(s_{t+1}|s_t,a_t)$所以直接忽略掉，$R(\tau^n)$应该放在积分号中间也可以。</p>
<p>我们可以看到这个梯度与$\theta$有关，所以每次更新梯度，都需要重新采样N次，也就是需要与环境交互，跑一个$\tau$出来，然后更新梯度，然后就需要重新再交互，这相当于是on-policy，换成off-policy就是PPO。</p>
<p>进行ppo之前先有两个tips。</p>
<h4 id="tip-1-add-a-base-line"><a href="#tip-1-add-a-base-line" class="headerlink" title="tip 1: add a base line"></a>tip 1: add a base line</h4><p>PG方法在更新策略时，基本思想就是增加reward大的动作出现的概率，减小reward小的策略出现的概率。假设现在有一种情况，我们的reward在无论何时都是正的，对于没有采样到的动作，它的reward是0。因此，如果一个比较好的动作没有被采样到，而采样到的不好的动作得到了一个比较小的正reward，那么没有被采样到的好动作的出现概率会越来越小，这显然是不合适的，因此我们需要增加一个奖励的基线，让reward有正有负。 增加的基线是所获得奖励的平均值b。</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583136617036.png" alt="1583136617036" style="zoom:33%;"></p>
<h4 id="tip-2-Assign-Suitable-Credit"><a href="#tip-2-Assign-Suitable-Credit" class="headerlink" title="tip 2: Assign Suitable Credit"></a>tip 2: Assign Suitable Credit</h4><p>当前动作之后获得的奖励才重要</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583137118284.png" alt="1583137118284" style="zoom:33%;"></p>
<p>添加一个discount，前面那个系数可以用Advantage Function表示：$A^\theta(s_t,a_t)$</p>
<p>这个相当于在时间t之后得到的<em>价值总和</em>（忘了专业名词叫啥了）</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583137342234.png" alt="1583137342234" style="zoom:33%;"></p>
<h3 id="Proximal-Policy-Optimization"><a href="#Proximal-Policy-Optimization" class="headerlink" title="Proximal Policy Optimization"></a>Proximal Policy Optimization</h3><p>采用重要性采样</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583137476846.png" alt="1583137476846" style="zoom:33%;"></p>
<p>可以从p的分布采样，换到从q的分布采样，需要注意，p和q的分布差异不能太大，因为虽然上式的均值相同，但是方差不同，还是会有影响。</p>
<script type="math/tex; mode=display">
\bigtriangledown\overline{R_\theta}= \frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_n}R(\tau^n)\bigtriangledown{\log{p_\theta (a_t^n|s_t^n)}}</script><p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583151524298.png" alt="1583151524298" style="zoom:33%;"><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583137724390.png" alt="1583137724390" style="zoom:33%;"></p>
<p>这里为什么是从$\pi_\theta$中sample出$(s_t,a_t)$怪不清楚的，现在的理解是</p>
<p>我们需要控制$\theta$和$\theta’$相似，使用KL divergence，这里肯定比较的是$p_\theta(\bullet|s)$和$p_{\theta’}(\bullet|s)$的距离</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583151733566.png" alt="1583151733566" style="zoom:33%;"></p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583151778726.png" alt="1583151778726" style="zoom:33%;"></p>
<p>算法流程：</p>
<ul>
<li>Initial policy parameters $\theta^0$</li>
<li>In each iteration <ul>
<li>Using $\theta^k$ to interact with the environment to collect $s_t,a_t$  and compute advantage $A^{\theta^k}(s_t,a_t)$</li>
<li>Find $\theta$ optimizeing $J^{\theta^k}(\theta)-\beta KL(\theta,\theta^k)$ <img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583152327757.png" alt="1583152327757" style="zoom: 50%;"></li>
</ul>
</li>
</ul>
<p>上面这些 来自于<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2018/Lecture/PPO (v3" target="_blank" rel="noopener">李宏毅课程</a>.pdf) 。</p>
<p>有几点没有懂</p>
<ul>
<li>新的策略需要凑一条条完整的流程$\tau$吗</li>
<li>advantage function 应该怎么计算</li>
<li>为什么要优化函数J</li>
</ul>
<p>更正：</p>
<p>重新看了下课程，之前弄错了，$\theta’$才是与环境交互的:smile:</p>
<p>优化的J的原因是，$\bigtriangledown f(x) = f(x)\bigtriangledown \log{f(x)}$相当于原函数是f(x)，也就是这里的J</p>
<p>在算kl divergence的时候，把所有sample到的$s_t$都算下两个model的输出的区别，这个好像还比较麻烦，具体怎么处理还不清楚</p>
<h4 id="TRPO"><a href="#TRPO" class="headerlink" title="TRPO"></a>TRPO</h4><p>简单写下PPO的前身，更麻烦的处理方式，TRPO讲KL divergence项作为一个constrain，不在optimize项里面。</p>
<h4 id="PPO2"><a href="#PPO2" class="headerlink" title="PPO2"></a>PPO2</h4><p>PPO2 提出了不用计算KL Divergence的方法</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583206637985.png" alt="1583206637985" style="zoom:67%;"></p>
<p>clip的意思是如果$p_\theta$和$p_{\theta’}$ 的比值如果小于$1-\varepsilon$则等于$1-\varepsilon$。</p>
<p>图中横轴是clip里的第一项纵轴是输出结果。</p>
<p>具体算法流程：</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583284441674.png" alt="1583284441674"></p>
<p>可以看到在训练过程中，是需要再学一个value function去估测advantage的。</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
  </entry>
</search>
