<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>PER</title>
    <url>/2020/06/05/PER/</url>
    <content><![CDATA[<p>paper:</p>
<p><a href="https://arxiv.org/abs/1511.05952" target="_blank" rel="noopener">https://arxiv.org/abs/1511.05952</a> </p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Experience transitions were uniformly, sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance.   </p>
<a id="more"></a>
<h3 id="Main-idea"><a href="#Main-idea" class="headerlink" title="Main idea:"></a>Main idea:</h3><p>Some transitions may not be immediately useful to the agent, They propose to more frequently replay transitions with high expected learning progress, as measured by the <strong>magnitude of their TD error</strong>.  This prioritization can lead to <strong>a loss of diversity</strong>, which they alleviate with stochastic prioritization, and introduce bias, which they correct with <strong>importance sampling</strong>.   </p>
<h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods:"></a>Methods:</h3><p>Goal is choosing which experience to replay.</p>
<h4 id="example-‘Blind-Cliffwalk’"><a href="#example-‘Blind-Cliffwalk’" class="headerlink" title="example:  ‘Blind Cliffwalk’"></a>example:  ‘Blind Cliffwalk’</h4><p><img src="/2020/06/05/PER/1591347439913.png" alt="1591347439913" style="zoom:50%;"></p>
<p><strong>reward</strong>: The green arrow reward is 1, others 0</p>
<p><strong>action</strong>: “wrong”: The dashed arrow, and the episode is terminated whenever the agent takes the ‘wrong’ action.</p>
<p>“right”:   Taking the ‘right’ action progresses through a sequence of n states (black arrows), at the end of which lies a final reward of 1 (green arrow)  </p>
<p><strong>Feature</strong>: the successes are rare.</p>
<p><strong>Uniform agent and oracle agent</strong>:</p>
<p>Uniform agent sample form buffer,</p>
<p>Oracle greedily selects the transition that maximally reduces the global loss in its current state (in hindsight, after the parameter update).  (not realistic)</p>
<p><img src="/2020/06/05/PER/1591358362455.png" alt="1591358362455" style="zoom:67%;"></p>
<p>Median number of learning steps required to learn the value function as a function of the size of the total<br>number of transitions in the replay memory.   </p>
<h4 id="PRIORITIZING-WITH-TD-ERROR"><a href="#PRIORITIZING-WITH-TD-ERROR" class="headerlink" title="PRIORITIZING WITH TD-ERROR"></a>PRIORITIZING WITH TD-ERROR</h4><p>The tuple stored is (st,at,rt,st+1,TD-error)</p>
<p>When we use Q-learning, we would get TD-error. </p>
<p>New transitions arrive without a known TD-error, so we put them at maximal priority in order to guarantee that all experience is seen at least once.</p>
<p><img src="/2020/06/05/PER/1591362874248.png" alt="1591362874248" style="zoom:60%;">  </p>
<p>this algorithm results in a substantial reduction in the effort required to solve the Blind Cliffwalk task.</p>
<h4 id="STOCHASTIC-PRIORITIZATION"><a href="#STOCHASTIC-PRIORITIZATION" class="headerlink" title="STOCHASTIC PRIORITIZATION"></a>STOCHASTIC PRIORITIZATION</h4><p>Greedy prioritization is lack of diversity that makes the system prone to over-fitting.</p>
<p>Stochastic prioritization’s probability:</p>
<p><img src="/2020/06/05/PER/1591412006698.png" alt="1591412006698" style="zoom:67%;"> </p>
<p>Two variant:</p>
<p>1)</p>
<p><img src="/2020/06/05/PER/1591412121080.png" alt="1591412121080" style="zoom:67%;"></p>
<p>2)</p>
<p><img src="/2020/06/05/PER/1591412202069.png" alt="1591412202069" style="zoom: 80%;"></p>
<p><img src="/2020/06/05/PER/1591412465369.png" alt="1591412465369" style="zoom:67%;"></p>
<h4 id="ANNEALING-THE-BIAS"><a href="#ANNEALING-THE-BIAS" class="headerlink" title="ANNEALING THE BIAS"></a>ANNEALING THE BIAS</h4><p>Prioritized replay introduces bias, and bias can be corrected this bias by using importance-sampling (IS) weights </p>
<p><img src="/2020/06/05/PER/1591444087261.png" alt="1591444087261" style="zoom:50%;"></p>
<p>Update Q value using wiδi instead of δi.</p>
<p>In practice, we linearly anneal β from its initial value β0 to 1.   </p>
<h4 id="ALGORITHM"><a href="#ALGORITHM" class="headerlink" title="ALGORITHM"></a>ALGORITHM</h4><p><img src="/2020/06/05/PER/1591447080765.png" alt="1591447080765"></p>
<h3 id="ATARI-EXPERIMENTS"><a href="#ATARI-EXPERIMENTS" class="headerlink" title="ATARI EXPERIMENTS"></a>ATARI EXPERIMENTS</h3><p>Average score per episode  </p>
<p><img src="/2020/06/05/PER/1591496100348.png" alt="1591496100348"></p>
<p>The learning speed  </p>
<p><img src="/2020/06/05/PER/1591496215490.png" alt="1591496215490"></p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
  </entry>
  <entry>
    <title>FairnessIsNotStatic</title>
    <url>/2020/05/06/FairnessIsNotStatic/</url>
    <content><![CDATA[<h2 id="Fairness-Is-Not-Static"><a href="#Fairness-Is-Not-Static" class="headerlink" title="Fairness Is Not Static"></a>Fairness Is Not Static</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation:"></a>Motivation:</h3><p>当前的Fairness都是静态的，数据都是fixed，而长期动态的环境中，现有的公平算法可能并不适用。</p>
<h3 id="主要内容："><a href="#主要内容：" class="headerlink" title="主要内容："></a>主要内容：</h3><p>主要介绍了三个 <a href="https://github.com/google/ml-fairness-gym" target="_blank" rel="noopener">ml-fairness-gym</a> 的仿真环境，通过实验与静态的方法对比，发现静态方法在动态环境中可能会有一些问题。</p>
<a id="more"></a>
<h3 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h3><h4 id="场景一：Lending"><a href="#场景一：Lending" class="headerlink" title="场景一：Lending"></a>场景一：Lending</h4><p>银行设定阈值来作为是否借款的依据，探究不同的策略带来的长期影响和对不同群体的公平影响。</p>
<p><strong>Environment</strong>：借款申请人</p>
<p>两个群体，一个优势，一个劣势。</p>
<p>可观测群体属性，离散的<strong>信用评分</strong>$C\in1,2,3,..,C_{max}$，假定评分到还款概率$\pi(C)$的映射是固定的，优势群体信用评分的分布要高点。</p>
<p>成功还款的话，用户信用上涨c+，agent收益上升r+，如果拖欠，信用下降c-，agent收益下降c-。</p>
<p><strong>Metrics</strong>：信用分布，累计的借贷，总真阳率</p>
<p><strong>agent</strong>：银行，根据信用评分决定是否借款。</p>
<p>两种agent：</p>
<ol>
<li><p>max-reward</p>
<p> 寻找阈值最大收益。</p>
<p> 因为收益的r+和r-都是定值，所以只要期望大于0应该就可以借，$\pi(C)&gt;\tau$。所以阈值是fixed的。</p>
<p> 前200轮全都借，然后找到固定的阈值。</p>
</li>
<li><p>EO</p>
<p> 对不同的群体使用不同的阈值，在每一步中，在不同群体TPR相同的限制下最大化收益。用随机插值将ROC补成连续。（ROC如何得到是个问题）</p>
</li>
</ol>
<p>还有很多细节还是得看代码里怎么写的</p>
<p><strong>实验结果</strong></p>
<p>实验是简化的，EO agent是知道当前信用分布的和对应的概率的，那么阈值-真阳率的图像就好做了</p>
<p>初始分布如下：</p>
<p><img src="/2020/05/06/FairnessIsNotStatic/1588856812650.png" alt="1588856812650" style="zoom: 33%;"></p>
<p>max-reward的方法的结果如下。在论文里有分析，如果经过无穷轮，因为信用一旦低于阈值就永久不再能借款，会导致所有的都到阈值以下。</p>
<p><img src="/2020/05/06/FairnessIsNotStatic/1588856851070.png" alt="1588856851070" style="zoom:33%;"></p>
<p>EO的结果如下：</p>
<p><img src="/2020/05/06/FairnessIsNotStatic/1588857016286.png" alt="1588857016286" style="zoom:33%;"></p>
<p>EO相对来说分布的差距还变得更大了。</p>
<p>累计的贷款数量和群体的还款概率如下：</p>
<p><img src="/2020/05/06/FairnessIsNotStatic/1588857167690.png" alt="1588857167690" style="zoom:50%;"></p>
<p>可以看出EO agent会给弱势群体更多的福利，但是群体的信用，弱势群体下降的更快。</p>
<p>下图是EO agent的阈值的设定，和正阳率的一个趋势图。</p>
<p><img src="/2020/05/06/FairnessIsNotStatic/1588858013328.png" alt="1588858013328" style="zoom:50%;"></p>
<p>可以看到，对于群体2的阈值设定要更低一点。</p>
<p>而且TPR即使在每一轮中都相同，最后总的TPR也是不同的，如下图。</p>
<p><img src="/2020/05/06/FairnessIsNotStatic/1588858182023.png" alt="1588858182023"></p>
<h4 id="场景二：Attention-Allocation"><a href="#场景二：Attention-Allocation" class="headerlink" title="场景二：Attention  Allocation"></a>场景二：Attention  Allocation</h4><p>Agent分配有限的注意力到多个不同的地方，每个地方有不同的发生事故的概率，探究不同策略带来的长期影响</p>
<p><strong>Environment</strong>：不同地点，每个地点发生事故的数量满足Poisson分布</p>
<p>agent要分配N个注意，有K个地方，每个时间t每个地方发生事故的数目为$y_{kt} \sim \rm{Poisson}(r_k,t)$   </p>
<p>发现的数目为$\hat{y}<em>{kt}:=\rm{min}(a</em>{kt},y_{kt})$</p>
<p>考虑动态情况的话，发生的概率会随着attention的数目改变，$d$为常数</p>
<p><img src="/2020/05/06/FairnessIsNotStatic/1588900713333.png" alt="1588900713333" style="zoom:50%;"></p>
<p><strong>Metrics</strong>：</p>
<p>总的发现数目，总的错过数目（真实世界中比较难得到）</p>
<p><img src="/2020/05/06/FairnessIsNotStatic/1588900949529.png" alt="1588900949529" style="zoom: 50%;"></p>
<p>作为公平度量。</p>
<p><strong>Agent</strong>：</p>
<p>均匀分配的<em>uniform agent</em></p>
<p><em>proportional agents</em>通过对$r$的估计值$\hat{r}$，探索策略epsilon-greedy，$\epsilon$的概率均匀分配，$1-\epsilon$的概率按照原计划</p>
<p>fairness-constrained <em>greedy agent</em>，sequentially的分配，最大化 the probability that the next unit of attention will result in a discovery ， 限制the maximum gap in discovery probabilities   between sites小于$\alpha$</p>
<p><strong>实验结果</strong></p>
<p>5地方$[8,6,4,3,1.5]$，6个attention</p>
<p><img src="/2020/05/06/FairnessIsNotStatic/1588902753228.png" alt="1588902753228"></p>
<p>静态环境表现差不多，动态环境中，purely greedy发现高，错过的也高，所以并不能只把发现的作为优化目标。</p>
<p><img src="/2020/05/06/FairnessIsNotStatic/1588905665078.png" alt="1588905665078">purely greedy会过多分配注意在刚刚发现了事故的地方</p>
<p>公平的测量如下</p>
<p><img src="/2020/05/06/FairnessIsNotStatic/1588907283086.png" alt="1588907283086"></p>
<p>uniform的表现较差，proportional epsilon=0.1和greedy alpha=0.75效果比较好</p>
<h4 id="场景三：college-admission"><a href="#场景三：college-admission" class="headerlink" title="场景三：college admission"></a>场景三：college admission</h4><p>学校要公布选择的规则，申请者可以cost to（可以理解为花费精力）改变分数。是一个Stackleberg博弈。</p>
<p>探索repeated play和以前的one-shot方法达到的均衡的差别。</p>
<p>鲁棒性的策略可能会带来公平问题，合格的人也需要发费精力，而且不同的群体的花费是不同的。</p>
<p><strong>Environment</strong>： agent公布阈值分数$\tau$，环境产生带有ground truth的申请者，这些申请者是理性的，只有改分数能去理想的学校而且cost不是过高才会改分数。</p>
<p><strong>Agent</strong>：目标是最大化准确度。</p>
<p><em>static agent</em>前几轮全接受，然后训练一个分类器，使用未更改的(score, label)对，这个label应该是是否合格，但是agent是否知道没说。</p>
<p><em>robust agent</em>使用以前论文的算法获得一个robust的agent，分类准确率应该是可以达到和未改分情况下一样的准确率。</p>
<p><em>continuous agent</em>  gathers an initial set of unmanipulated applicants, then continuously retrains a non-robust classifier based on the subsequent manipulated scores and labels that it observes.  </p>
<p><strong>实验结果</strong></p>
<p><img src="/2020/05/06/FairnessIsNotStatic/1588927276507.png" alt="1588927276507"></p>
<p>竖着的几条对应着阈值大小</p>
<p>其他的曲线表示阈值与准确率，社会负担（合格者需要的花费）的关系，左边是分数和合格之间的关系没有噪声，右边是有噪声。</p>
<p>可以看到有噪声的情况，即使<em>continuous agent</em>未考虑鲁棒性，结果与鲁棒性agent的结果是一样的。</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
  </entry>
  <entry>
    <title>Q-learning（1）</title>
    <url>/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/</url>
    <content><![CDATA[<h2 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h2><p>李宏毅老师主页18年机器学习教程： <a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html" target="_blank" rel="noopener">http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html</a> （老师的2020年视频也出了）</p>
 <a id="more"></a> 
<h3 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h3><p>值函数分为： State value function ， State-action value function </p>
<p>状态值函数$V_\pi(s)$，是对状态s的估计</p>
<h4 id="如何估计状态值函数"><a href="#如何估计状态值函数" class="headerlink" title="如何估计状态值函数"></a>如何估计状态值函数</h4><p><strong>monte-carlo</strong>方法：</p>
<p>对多个episode求平均，可以用神经网咯拟合</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586575571217.png" alt="1586575571217"></p>
<p><strong>TD</strong>方法：</p>
<p>两个状态之间的值函数之差是reward，拟合reward就可以了</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586575379396.png" alt="1586575379396"></p>
<p>注：MC方法相较来说<strong>方差</strong>更大，并且这两种估计方法可能结果不一样大</p>
<h3 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h3><p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586576088554.png" alt="1586576088554" style="zoom: 33%;"></p>
<p>对于一个状态，选择一个最大的Q func就可以了</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586585731110.png" alt="1586585731110" style="zoom:33%;"></p>
<p>证明：</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586585767328.png" alt="1586585767328" style="zoom:33%;"></p>
<h4 id="三个技巧"><a href="#三个技巧" class="headerlink" title="三个技巧"></a>三个技巧</h4><p><strong>Target network</strong></p>
<p>由于对q func的拟合是用的两个q之差与reward的比较，所以需要固定住一个</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586585935056.png" alt="1586585935056" style="zoom: 33%;"></p>
<p><strong>Exploration</strong></p>
<p>策略更新的方式是选择q(s,a)最大的，这在前面阶段肯定是不合适的，因为很多(s,a)没有被遍历过。</p>
<p>两种解决方法如下图</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586586846153.png" alt="1586586846153" style="zoom:33%;"></p>
<p><strong>replay buffer</strong></p>
<p>弄一个缓存，里面放（st,at,rt,st+1）</p>
<p>每次从buffur里取一个batch，然后训练q func</p>
<h4 id="Typical-Q-Learning-Algorithm"><a href="#Typical-Q-Learning-Algorithm" class="headerlink" title="Typical Q-Learning Algorithm"></a>Typical Q-Learning Algorithm</h4><p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586587733417.png" alt="1586587733417"></p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
  </entry>
  <entry>
    <title>fairness-gym搭建</title>
    <url>/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p> github网址<a href="https://github.com/google/ml-fairness-gym" target="_blank" rel="noopener">https://github.com/google/ml-fairness-gym</a> </p>
<a id="more"></a>
<p>一个仿真环境</p>
<p><a href="https://github.com/google/ml-fairness-gym/blob/master/papers/acm_fat_2020_fairness_is_not_static.pdf" target="_blank" rel="noopener">Fairness is not Static: Deeper Understanding of Long Term Fairness via Simulation Studies</a></p>
<ul>
<li>College admissions</li>
<li>Lending</li>
<li>Attention allocation</li>
</ul>
<p><a href="https://github.com/google/ml-fairness-gym/blob/master/papers/fairmlforhealth2019_fair_treatment_allocations_in_social_networks.pdf" target="_blank" rel="noopener">Fair treatment allocations in social networks</a></p>
<ul>
<li>Infectious disease</li>
</ul>
<h2 id="场景一：-LENDING"><a href="#场景一：-LENDING" class="headerlink" title="场景一：  LENDING"></a>场景一：  LENDING</h2><p>(这个场景相关的设定有三种不同的，gym中文档中最重要，另外两篇论文里的辅助理解)</p>
<p><strong>Delayed Impact of Fair Machine Learning</strong>这篇文章是one-step的，gym提供的是 many steps  </p>
<p>个人被标记一个信用分数$\mathcal{X}:={1,2,…,C}$，分数越高表示越有可能带来正向收益<del>，我的理解是越有可能还上款</del>。</p>
<p>银行采用一个策略$\tau:\mathcal{X}\rightarrow[0,1]$表示选择的概率，我的理解是策略是对一个固定的分数，借钱的概率是相同的。</p>
<p><img src="/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/1587195490449.png" alt="1587195490449" style="zoom: 50%;"></p>
<p>考虑两个群体，A、B，一个优势，一个劣势，分数分布为$\pi_A$，$\pi_B$</p>
<p>A群体占总体人数的$g_A$</p>
<p><img src="/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/1587212231669.png" alt="1587212231669" style="zoom:50%;"></p>
<p><del>没看懂这是个啥</del></p>
<p>例子中写了，如下</p>
<p><img src="/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/1587214193670.png" alt="1587214193670" style="zoom:50%;"></p>
<p>我的理解是这是赚多少钱</p>
<p><img src="/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/1587212998930.png" alt="1587212998930" style="zoom:50%;"></p>
<p><del>持续看不懂成功是指的成功借到钱？</del>看后面的例子说是一个人在固定时间内还钱的概率</p>
<p><img src="/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/1587214352552.png" alt="1587214352552" style="zoom:50%;"></p>
<p>还钱后分数增加，不换分数减少</p>
<hr>
<p><strong>Fairness is not static</strong>中的说明</p>
<p>C表示分数</p>
<p>$\pi(C)$表示成功还款率</p>
<p>$p_{0}^{A}(C)$为A群体的C的分布</p>
<hr>
<p><strong>文档中的说明</strong></p>
<p> <a href="https://github.com/google/ml-fairness-gym/blob/master/docs/quickstart.md" target="_blank" rel="noopener">https://github.com/google/ml-fairness-gym/blob/master/docs/quickstart.md</a> </p>
<p><img src="/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/1587296941877.png" alt="1587296941877"></p>
<p>文档里这段之后给出了一些衡量的说明，但是看了也不知道怎么写自己的策略，现在能想到的解决办法是学习下python的debug，一步一步看下实例代码跑了哪些</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python experiments&#x2F;lending_demo.py --num_steps&#x3D;5000</span><br></pre></td></tr></table></figure>
<p> The classifier is trying to find one or more appropriate threshold(s) for giving out loans. It will follow a simple rule. For the first 200 steps, it will give out loans to everyone. After that, it will use that data to find the threshold with the <strong>highest accuracy</strong>. (In this simulation, the cost of a false positive is exactly equal to the gain from a true positive, so accuracy is a reasonable metric to optimize). </p>
<p>（这里的最高的准确率怎么理解不清楚</p>
<p>手动分割，看了下<strong>lending_demo.py</strong>代码似乎还是比较易读</p>
<p><code>__future__</code>包和<code>absl</code>之前没用过，可能稍微学一下， <a href="https://www.jianshu.com/p/2140b519028d" target="_blank" rel="noopener">https://www.jianshu.com/p/2140b519028d</a> 和</p>
<p> <a href="https://abseil.io/docs/" target="_blank" rel="noopener">https://abseil.io/docs/</a>   <a href="http://www.attrs.org/en/stable/" target="_blank" rel="noopener">http://www.attrs.org/en/stable/</a> </p>
<p>下面先记录下重要的代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># experiments/lending_demo.py</span></span><br><span class="line">result = lending.Experiment(</span><br><span class="line">    <span class="comment"># 0.5</span></span><br><span class="line">    group_0_prob=group_0_prob,</span><br><span class="line">    <span class="comment"># 利率</span></span><br><span class="line">    interest_rate=<span class="number">1.0</span>,</span><br><span class="line">    bank_starting_cash=<span class="number">10000</span>,</span><br><span class="line">    seed=<span class="number">200</span>,</span><br><span class="line">    num_steps=FLAGS.num_steps,</span><br><span class="line">    <span class="comment"># 我猜是For the first 200 steps, it will give out loans to everyone.</span></span><br><span class="line">    burnin=<span class="number">200</span>,</span><br><span class="line">    <span class="comment"># 暂时不知道是啥</span></span><br><span class="line">    cluster_shift_increment=<span class="number">0.01</span>,</span><br><span class="line">    include_cumulative_loans=<span class="literal">True</span>,</span><br><span class="line">    return_json=<span class="literal">False</span>,</span><br><span class="line">    <span class="comment"># policy就是在这个写</span></span><br><span class="line">    <span class="comment"># MAXIMIZE_REWARD = threshold_policies.ThresholdPolicy.MAXIMIZE_REWARD</span></span><br><span class="line">	<span class="comment"># EQUALIZE_OPPORTUNITY = threshold_policies.ThresholdPolicy.EQUALIZE_OPPORTUNITY</span></span><br><span class="line">    threshold_policy=(EQUALIZE_OPPORTUNITY <span class="keyword">if</span> FLAGS.equalize_opportunity <span class="keyword">else</span></span><br><span class="line">                      MAXIMIZE_REWARD)).run()</span><br></pre></td></tr></table></figure>
<p>先看看最简单的<code>MAXIMIZE_REWARD</code>是怎么写的</p>
<p>在agents/threshold_policies.py中看到MAXIMIZE_REWARD只是个枚举类</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ThresholdPolicy</span><span class="params">(enum.Enum)</span>:</span></span><br><span class="line">  SINGLE_THRESHOLD = <span class="string">"single_threshold"</span></span><br><span class="line">  MAXIMIZE_REWARD = <span class="string">"maximize_reward"</span></span><br><span class="line">  EQUALIZE_OPPORTUNITY = <span class="string">"equalize_opportunity"</span></span><br></pre></td></tr></table></figure>
<p>lending中的Experiment对象所有的变量都是<code>attr.ib()</code>，暂时不影响理解</p>
<p>Experiment对象首先给了三种参数，env的参数，agent的参数，run的参数</p>
<p>然后定义了两个函数，<code>scenario_builder</code>和<code>run</code></p>
<p><code>run</code>里面调用<code>scenario_builder</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">env = lending.DelayedImpactEnv(env_params)</span><br></pre></td></tr></table></figure>
<p>这里调用的lending是environment中的</p>
<p><img src="/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/1587440760162.png" alt="1587440760162" style="zoom:50%;"></p>
<p>这里定义的lending的环境有这几种</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">agent = oracle_lending_agent.OracleThresholdAgent(</span><br><span class="line">            action_space=env.action_space,</span><br><span class="line">            reward_fn=rewards.BinarizedScalarDeltaReward(</span><br><span class="line">                <span class="string">'bank_cash'</span>, baseline=env.initial_params.bank_starting_cash),</span><br><span class="line">            observation_space=env.observation_space,</span><br><span class="line">            params=agent_params,</span><br><span class="line">            env=env)</span><br></pre></td></tr></table></figure>
<p>Threshold agent with oracle access to distributional data.</p>
<p>OracleThresholdAgent的父类是classifier_agents.ThresholdAgent，他的父类是ScoringAgent，他的父类是core.Agent</p>
<p>agent这里似乎是一些参数的定义，具体的agent的算法似乎是没有看到，reward的算法定义了，reward的func就几行暂时不看，下面看run</p>
<p>看了下重要的代码应该是这个</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">metric_results = run_util.run_simulation(env, agent, metrics,                                         self.num_steps, self.seed)</span><br></pre></td></tr></table></figure>
<p>然后找到</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">action = agent.act(observation, done)</span><br></pre></td></tr></table></figure>
<p>用pycharm debug到这个agent的base class是ScoringAgent</p>
<p>agent.act调用的是ScoringAgent 中_act_impl</p>
<h2 id="场景2-入学"><a href="#场景2-入学" class="headerlink" title="场景2 入学"></a>场景2 入学</h2><p>看文章fairness is not static，入学这段开篇猛击，给钱改分？</p>
<p> in which individuals are able to <strong>pay a cost to manipulate their features</strong> in order to obtain a desired decision from the agent.   </p>
<p>个人知道agent的rule，可以 e.g., by investing in test prepcourses  </p>
<p><strong>暂时理解，agent的rule是知道的，申请者可以花费去改分，agent可以预测更改，然后实施一个robust的决策</strong></p>
<p>if the agent has knowledge of the cost functions so that it can determine how much applicants must pay to manipulate their scores, then the agent can learn a best-response decision rule that nearly recovers the accuracy of the optimal decision rule on unmanipulated scores. </p>
<p>这段谷歌翻译翻译的recover是恢复，这样就可以理解了</p>
<p>robust的决策可能比较保守，使得qualified的人改分数，不qualified的人too costly（这样好像是好的）</p>
<p>但是可能对于disadvantage的群体，合格者可能会花不成比例的钱</p>
<p>trade-off的是一边要升高分类的准确，一边要减少qualified的申请人（文中这里没指定是弱势群体比较奇怪）</p>
<p>研究目标：</p>
<ol>
<li>单步和长期训练的区别</li>
<li>unmanipulated score and their true label之间的noise对于长期训练的影响</li>
</ol>
<p>结论：</p>
<p>if manipulation is occurring, practitioners should consider the fairness implications of robust classifcation<br>even if they are not deploying a robust agent.  </p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
  </entry>
  <entry>
    <title>python面向对象</title>
    <url>/2020/04/21/python%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/</url>
    <content><![CDATA[<p>看fair gym然后发现我对python里的继承的用法有些误解</p>
<a id="more"></a>
<p>使用教程 <a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017495723838528" target="_blank" rel="noopener">https://www.liaoxuefeng.com/wiki/1016959663602400/1017495723838528</a> </p>
<p>想要学到知识</p>
<ul>
<li>[x] 继承是继承哪些东西，方法和属性？</li>
<li>[x] 有没有private类</li>
<li>[ ] 怎么选择继承</li>
</ul>
<h2 id="class"><a href="#class" class="headerlink" title="class"></a>class</h2><p>所有类最终的父类都是object</p>
<p> 如果要让内部属性不被外部访问，可以把属性的名称前加上两个下划线<code>__</code>，在Python中，实例的变量名如果以<code>__</code>开头，就变成了一个私有变量（<strong>private</strong>），只有内部可以访问，外部不能访问 </p>
<p>看到 <a href="https://blog.csdn.net/brucewong0516/article/details/79119551" target="_blank" rel="noopener">https://blog.csdn.net/brucewong0516/article/details/79119551</a> 中提到<strong>实例方法、静态方法和类方法</strong> </p>
<p><strong>实例方法</strong></p>
<p>第一个参数self，可以通过self访问类属性，这个好像我自己常用的方法</p>
<p><strong>静态方法</strong></p>
<p>定义的时候使用<code>@staticmethod</code>装饰器，打开了新世界，。。。不需要实例参数也不需要类参数</p>
<p>看起来很迷幻。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">person</span><span class="params">(object)</span>:</span></span><br><span class="line">    tall = <span class="number">180</span></span><br><span class="line">    hobbies = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age,weight)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.weight = weight</span><br><span class="line"><span class="meta">    @staticmethod    #静态方法装饰器</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">infoma</span><span class="params">()</span>:</span>     <span class="comment">#没有参数限制，既不要实例参数，也不用类参数</span></span><br><span class="line">        print(person.tall)</span><br><span class="line">        print(person.hobbies)</span><br><span class="line"><span class="comment">#person.infoma()   #静态法可以通过类名访问</span></span><br><span class="line">Bruce = person(<span class="string">"Bruce"</span>, <span class="number">25</span>,<span class="number">180</span>)   <span class="comment">#通过实例访问</span></span><br><span class="line">Bruce.infoma()</span><br></pre></td></tr></table></figure>
<p><strong>类方法</strong></p>
<p>类方法<strong>以cls作为第一个参数</strong>，<strong>cls表示类本身</strong>，定义时<strong>使用@classmethod装饰器</strong>。通过cls可以访问类的相关属性。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">person</span><span class="params">(object)</span>:</span></span><br><span class="line">    tall = <span class="number">180</span></span><br><span class="line">    hobbies = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age,weight)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.weight = weight</span><br><span class="line"><span class="meta">    @classmethod     #类的装饰器</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">infoma</span><span class="params">(cls)</span>:</span>   <span class="comment">#cls表示类本身，使用类参数cls</span></span><br><span class="line">        print(cls.__name__)</span><br><span class="line">        print(dir(cls))</span><br><span class="line"><span class="comment">#cls表示类本身</span></span><br><span class="line"><span class="comment">#person.infoma()  直接调用类的装饰器函数，通过cls可以访问类的相关属性</span></span><br><span class="line">Bruce = person(<span class="string">"Bruce"</span>, <span class="number">25</span>,<span class="number">180</span>)   <span class="comment">#也可以通过两步骤来实现，第一步实例化person，第二步调用装饰器</span></span><br><span class="line">Bruce.infoma()</span><br></pre></td></tr></table></figure>
<p>就像学了假python</p>
<h2 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h2><p>继承很简单，在定义子类的时候把父类放在括号里就可以</p>
<p>父类的方法和属性都会被继承</p>
<p>子类自定义属性、方法可以覆盖父类，实现多态</p>
<p>这个时候如果要访问父类的属性，可以使用直接通过父类名调用，需要把self传进去不过（有点神奇，这个self应该是子类，相当于把子类传到了父类？）。</p>
<p>但是这样需要父类名硬编码到子类（忘了啥是硬编码了），所以推荐使用<code>super</code></p>
<p><code>super(Child,self)</code>，第一个参数子类，第二个参数父类</p>
<h2 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h2><p>这个东西很有趣，顺便学一下， <a href="https://foofish.net/python-decorator.html" target="_blank" rel="noopener">https://foofish.net/python-decorator.html</a> </p>
<p>装饰器本质上是一个 Python 函数或类，它可以让其他函数或类在不需要做任何代码修改的前提下增加额外功能，装饰器的返回值也是一个函数/类对象。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_logging</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">()</span>:</span></span><br><span class="line">        logging.warn(<span class="string">"%s is running"</span> % func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func()   <span class="comment"># 把 foo 当做参数传递进来时，执行func()就相当于执行foo()</span></span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'i am foo'</span>)</span><br><span class="line">foo = use_logging(foo)  <span class="comment"># 因为装饰器 use_logging(foo) 返回的时函数对象 wrapper，这条语句相当于  foo = wrapper</span></span><br><span class="line">foo()                   <span class="comment"># 执行foo()就相当于执行 wrapper()</span></span><br></pre></td></tr></table></figure>
<p>有点意思，层层套娃</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_logging</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">()</span>:</span></span><br><span class="line">        logging.warn(<span class="string">"%s is running"</span> % func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func()</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"><span class="meta">@use_logging</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"i am foo"</span>)</span><br><span class="line">foo()</span><br></pre></td></tr></table></figure>
<p>这里这个@就替代了上面的foo=use_logging(foo)</p>
<p>注意这里的foo()是没有参数的</p>
<p>如果有多个参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args)</span>:</span></span><br><span class="line">        logging.warn(<span class="string">"%s is running"</span> % func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure>
<p>  foo 函数还定义了一些关键字参数呢？ </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        <span class="comment"># args是一个数组，kwargs一个字典</span></span><br><span class="line">        logging.warn(<span class="string">"%s is running"</span> % func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure>
<p>奇怪的知识又增加了</p>
<p>装饰器还可以带参数（暂时不记。</p>
<hr>
<p>分割，上面讲的是函数的装饰器</p>
<p>还有<strong>类装饰器</strong></p>
<p> 使用类装饰器主要依靠类的<code>__call__</code>方法，当使用 @ 形式将装饰器附加到函数上时，就会调用此方法。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, func)</span>:</span></span><br><span class="line">        self._func = func</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'class decorator runing'</span>)</span><br><span class="line">        self._func()</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'class decorator ending'</span>)</span><br><span class="line"><span class="meta">@Foo</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'bar'</span>)</span><br><span class="line">bar()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title>The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems</title>
    <url>/2020/05/01/TheDynamicsofReinforcementLearninginCooperativeMultiagentSystems/</url>
    <content><![CDATA[<p>传送： <a href="https://www.aaai.org/Papers/AAAI/1998/AAAI98-106.pdf" target="_blank" rel="noopener">https://www.aaai.org/Papers/AAAI/1998/AAAI98-106.pdf</a> </p>
<a id="more"></a>
<p>多智体入门论文，之前看多智体的文章比较少且比较难看懂，找师兄推荐了篇比较早的简单的论文</p>
<p><strong>自己总结的摘要</strong>：在单state的场景，通过重复训练来学习Q value，探索了智能体能观察到彼此（JALs）和智能体只知道自己（ILs）两种多智能体系统使用q-learning的表现，在满足一定条件的情况下都可以达到纳什均衡，但是不保证收敛到最优的纳什均衡，论文给出新的探索方式可以让JALs达到纳什均衡。</p>
<p><strong>纳什均衡</strong>：对于每个智能体，在其他智能体不变策略的情况下，选择最优的策略</p>
<p><strong>探索策略</strong>：</p>
<p><strong>normal Boltzmann (NB)</strong>  :</p>
<p><img src="/2020/05/01/TheDynamicsofReinforcementLearninginCooperativeMultiagentSystems/1588302887668.png" alt="1588302887668" style="zoom:50%;"></p>
<p>T随着训练降低</p>
<p>JALs的Q func</p>
<p><img src="/2020/05/01/TheDynamicsofReinforcementLearninginCooperativeMultiagentSystems/1588303227934.png" alt="1588303227934" style="zoom:50%;"></p>
<p>该方法不保证能到达最优</p>
<p><img src="/2020/05/01/TheDynamicsofReinforcementLearninginCooperativeMultiagentSystems/1588303328655.png" alt="1588303328655"></p>
<p>WOB和Combined方法更有可能到达最优（文章没有严格的证明）</p>
<p><strong>总结思考</strong>：考虑公平在这篇文章上没有太大用处，因为本文的场景所有agent只有一个reward，只需要让reward更大，所以更多的考虑探索过程，如何有效的探索并且能让agent达成共识，在这个场景下可能直接使用一个agent，把action分成多维度可能更好。</p>
]]></content>
  </entry>
  <entry>
    <title>强化基础复习</title>
    <url>/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/</url>
    <content><![CDATA[<p>看李宏毅老师的深度强化会感觉比较东西比较偏应用，正好要看多智体相关的东西，找了张伟楠老师的主页 <a href="http://wnzhang.net/" target="_blank" rel="noopener">http://wnzhang.net/</a> ，顺便复习下</p>
<a id="more"></a>
<h2 id="bellman-equation-for-value-function"><a href="#bellman-equation-for-value-function" class="headerlink" title="bellman equation for value function"></a>bellman equation for value function</h2><p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586868221816.png" alt="1586868221816"></p>
<p>$P_{s\pi}$ is state transition. At state s, use policy pi</p>
<p>Optimal value function:</p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586868369635.png" alt="1586868369635"></p>
<h2 id="值函数迭代-amp-策略迭代"><a href="#值函数迭代-amp-策略迭代" class="headerlink" title="值函数迭代&amp;策略迭代"></a>值函数迭代&amp;策略迭代</h2><p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586917042173.png" alt="1586917042173"></p>
<p>需要已知转移矩阵和reward function</p>
<p>这里的reward在这里应该是假设只依赖于状态s</p>
<p>动态规划：</p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586941139436.png" alt="1586941139436" style="zoom:33%;"></p>
<h2 id="model-free"><a href="#model-free" class="headerlink" title="model free"></a>model free</h2><h3 id="model-free-prediction"><a href="#model-free-prediction" class="headerlink" title="model-free prediction"></a>model-free prediction</h3><p><strong>MC</strong></p>
<script type="math/tex; mode=display">
G_t=R_{t+1}+\gamma R_{t+2}+...+\gamma^{T-1}R_{T}</script><script type="math/tex; mode=display">
V^{\pi}(s)=E[G_t|s_t=s,\pi]\approx \frac{1}{N}\sum_{i=1}^NG^{(i)}_{t}</script><p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586919493957.png" alt="1586919493957" style="zoom:33%;"></p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586940567481.png" alt="1586940567481" style="zoom:33%;"></p>
<p><strong>TD</strong></p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586919744733.png" alt="1586919744733" style="zoom:33%;"></p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586940746277.png" alt="1586940746277" style="zoom:33%;"></p>
<h3 id="model-free-control"><a href="#model-free-control" class="headerlink" title="model-free control"></a>model-free control</h3><p><strong>$\epsilon$-greedy</strong></p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1587038476938.png" alt="1587038476938" style="zoom:33%;"></p>
<p>前面的mc和td的目的是估计出值函数，<strong>control是为了得到最优的policy</strong></p>
<p><strong>mc control</strong></p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586954094046.png" alt="1586954094046" style="zoom:33%;"></p>
<p><strong>SARSA</strong></p>
<p>state-action-reward-state-action  </p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1587038173717.png" alt="1587038173717" style="zoom:33%;"></p>
<p>可以看到，这里是一个q的预测并不是control，但是q func得到之后就很容易得到pi，因为对于同一个s，选择最大的q(s,a)就可以了</p>
<p>在线学习版本：</p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1587039060349.png" alt="1587039060349"></p>
<p>似乎sarsa没有看到有用off-policy的，离线的可能就是q-learning了</p>
<p><strong>Q-learning</strong></p>
<p>可以参看之前的Q-learning（1）里面的q-learning，与本文的表示方式稍微有点不一样</p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1587040735589.png" alt="1587040735589" style="zoom:33%;"></p>
<p>这里的$\mu$是 交互的策略</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
  </entry>
  <entry>
    <title>强化学习经典论文</title>
    <url>/2020/06/04/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E8%AE%BA%E6%96%87/</url>
    <content><![CDATA[<h4 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h4><p><a href="https://b23.tv/yVNMMJ" target="_blank" rel="noopener">https://b23.tv/yVNMMJ</a></p>
<h4 id="DuelingDQN"><a href="#DuelingDQN" class="headerlink" title="DuelingDQN"></a>DuelingDQN</h4><p><a href="https://b23.tv/vL5GwU" target="_blank" rel="noopener">https://b23.tv/vL5GwU</a></p>
<h4 id="DRQN"><a href="#DRQN" class="headerlink" title="DRQN"></a>DRQN</h4><p><a href="https://b23.tv/hm2Sp3" target="_blank" rel="noopener">https://b23.tv/hm2Sp3</a></p>
<h4 id="DPG"><a href="#DPG" class="headerlink" title="DPG"></a>DPG</h4><p><a href="https://b23.tv/fcfmRJ" target="_blank" rel="noopener">https://b23.tv/fcfmRJ</a></p>
<h4 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h4><p><a href="https://b23.tv/JubzBq" target="_blank" rel="noopener">https://b23.tv/JubzBq</a></p>
<a id="more"></a>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
  </entry>
  <entry>
    <title>强化学习PPO</title>
    <url>/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/</url>
    <content><![CDATA[<h2 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h2><h3 id="basic"><a href="#basic" class="headerlink" title="basic"></a>basic</h3><p>每一个eposide记录为$\tau$</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/4155986-0da7e5f276ec5aca.webp" style="zoom:80%;"></p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583120692790.png" alt="1583120692790" style="zoom: 33%;"></p>
 <a id="more"></a> 
<p>$p_\theta(\tau)$是按策略参数为$\theta$得到这样一个trajectory的概率</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583120878285.png" alt="1583120878285" style="zoom: 33%;"></p>
<p>要提高这个期望，使用梯度上升，对$\theta$求梯度使用上式中的中间这个求和形式，使用技巧</p>
<script type="math/tex; mode=display">
\bigtriangledown f(x) = f(x)\bigtriangledown \log{f(x)}</script><p>这样可以转化为概率采样的形式，</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\bigtriangledown\overline{R_\theta} & =\bigtriangledown\sum_{\tau}{R(\tau)p_\theta(\tau)}
\\& = \sum_\tau{R(\tau)p_\theta(\tau)\bigtriangledown \log{p_\theta(\tau)}}
\\& = E_{\tau\sim p_\theta}(R(\tau)\bigtriangledown{\log{p_\theta(\tau)}})
\\& \approx \frac{1}{N}\sum_{n=1}^N{R(\tau^n)\bigtriangledown{\log{p_\theta(\tau^n)}}}
\\& = \frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_n}R(\tau^n)\bigtriangledown{\log{p_\theta (a_t^n|s_t^n)}}
\end{aligned}</script><p>最后一行中因为我们的梯度不关心$p(s_{t+1}|s_t,a_t)$所以直接忽略掉，$R(\tau^n)$应该放在积分号中间也可以。</p>
<p>我们可以看到这个梯度与$\theta$有关，所以每次更新梯度，都需要重新采样N次，也就是需要与环境交互，跑一个$\tau$出来，然后更新梯度，然后就需要重新再交互，这相当于是on-policy，换成off-policy就是PPO。</p>
<p>进行ppo之前先有两个tips。</p>
<h4 id="tip-1-add-a-base-line"><a href="#tip-1-add-a-base-line" class="headerlink" title="tip 1: add a base line"></a>tip 1: add a base line</h4><p>PG方法在更新策略时，基本思想就是增加reward大的动作出现的概率，减小reward小的策略出现的概率。假设现在有一种情况，我们的reward在无论何时都是正的，对于没有采样到的动作，它的reward是0。因此，如果一个比较好的动作没有被采样到，而采样到的不好的动作得到了一个比较小的正reward，那么没有被采样到的好动作的出现概率会越来越小，这显然是不合适的，因此我们需要增加一个奖励的基线，让reward有正有负。 增加的基线是所获得奖励的平均值b。</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583136617036.png" alt="1583136617036" style="zoom:33%;"></p>
<h4 id="tip-2-Assign-Suitable-Credit"><a href="#tip-2-Assign-Suitable-Credit" class="headerlink" title="tip 2: Assign Suitable Credit"></a>tip 2: Assign Suitable Credit</h4><p>当前动作之后获得的奖励才重要</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583137118284.png" alt="1583137118284" style="zoom:33%;"></p>
<p>添加一个discount，前面那个系数可以用Advantage Function表示：$A^\theta(s_t,a_t)$</p>
<p>这个相当于在时间t之后得到的<em>价值总和</em>（忘了专业名词叫啥了）</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583137342234.png" alt="1583137342234" style="zoom:33%;"></p>
<h3 id="Proximal-Policy-Optimization"><a href="#Proximal-Policy-Optimization" class="headerlink" title="Proximal Policy Optimization"></a>Proximal Policy Optimization</h3><p>采用重要性采样</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583137476846.png" alt="1583137476846" style="zoom:33%;"></p>
<p>可以从p的分布采样，换到从q的分布采样，需要注意，p和q的分布差异不能太大，因为虽然上式的均值相同，但是方差不同，还是会有影响。</p>
<script type="math/tex; mode=display">
\bigtriangledown\overline{R_\theta}= \frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_n}R(\tau^n)\bigtriangledown{\log{p_\theta (a_t^n|s_t^n)}}</script><p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583151524298.png" alt="1583151524298" style="zoom:33%;"><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583137724390.png" alt="1583137724390" style="zoom:33%;"></p>
<p>这里为什么是从$\pi_\theta$中sample出$(s_t,a_t)$怪不清楚的，现在的理解是</p>
<p>我们需要控制$\theta$和$\theta’$相似，使用KL divergence，这里肯定比较的是$p<em>\theta(\bullet|s)$和$p</em>{\theta’}(\bullet|s)$的距离</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583151733566.png" alt="1583151733566" style="zoom:33%;"></p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583151778726.png" alt="1583151778726" style="zoom:33%;"></p>
<p>算法流程：</p>
<ul>
<li>Initial policy parameters $\theta^0$</li>
<li>In each iteration <ul>
<li>Using $\theta^k$ to interact with the environment to collect $s_t,a_t$  and compute advantage $A^{\theta^k}(s_t,a_t)$</li>
<li>Find $\theta$ optimizeing $J^{\theta^k}(\theta)-\beta KL(\theta,\theta^k)$ <img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583152327757.png" alt="1583152327757" style="zoom: 50%;"></li>
</ul>
</li>
</ul>
<p>上面这些 来自于<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2018/Lecture/PPO (v3" target="_blank" rel="noopener">李宏毅课程</a>.pdf) 。</p>
<p>有几点没有懂</p>
<ul>
<li>新的策略需要凑一条条完整的流程$\tau$吗</li>
<li>advantage function 应该怎么计算</li>
<li>为什么要优化函数J</li>
</ul>
<p>更正：</p>
<p>重新看了下课程，之前弄错了，$\theta’$才是与环境交互的:smile:</p>
<p>优化的J的原因是，$\bigtriangledown f(x) = f(x)\bigtriangledown \log{f(x)}$相当于原函数是f(x)，也就是这里的J</p>
<p>在算kl divergence的时候，把所有sample到的$s_t$都算下两个model的输出的区别，这个好像还比较麻烦，具体怎么处理还不清楚</p>
<h4 id="TRPO"><a href="#TRPO" class="headerlink" title="TRPO"></a>TRPO</h4><p>简单写下PPO的前身，更麻烦的处理方式，TRPO讲KL divergence项作为一个constrain，不在optimize项里面。</p>
<h4 id="PPO2"><a href="#PPO2" class="headerlink" title="PPO2"></a>PPO2</h4><p>PPO2 提出了不用计算KL Divergence的方法</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583206637985.png" alt="1583206637985" style="zoom:67%;"></p>
<p>clip的意思是如果$p<em>\theta$和$p</em>{\theta’}$ 的比值如果小于$1-\varepsilon$则等于$1-\varepsilon$。</p>
<p>图中横轴是clip里的第一项纵轴是输出结果。</p>
<p>具体算法流程：</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583284441674.png" alt="1583284441674"></p>
<p>可以看到在训练过程中，是需要再学一个value function去估测advantage的。</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
  </entry>
  <entry>
    <title>强化学习读书（1.1-6.3）</title>
    <url>/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/</url>
    <content><![CDATA[<p><strong>Ch1.1-Ch1.6</strong>：崔昊川</p>
<p><a href="https://blog.csdn.net/qq_41608822/article/details/105902504" target="_blank" rel="noopener">https://blog.csdn.net/qq_41608822/article/details/105902504</a></p>
<p><strong>Ch2.1-Ch2.4</strong>：韦国梁</p>
<p>见正文</p>
<p><strong>Ch2.5-Ch2.8</strong>：崔昊川</p>
<p><a href="https://blog.csdn.net/qq_41608822/article/details/105928928" target="_blank" rel="noopener">https://blog.csdn.net/qq_41608822/article/details/105928928</a></p>
<p><strong>Ch3.1-Ch3.3</strong>：韦国梁</p>
<p>见正文</p>
<a id="more"></a>
<p><strong>Ch3.4-Ch3.8</strong>：崔昊川</p>
<p><a href="https://blog.csdn.net/qq_41608822/article/details/106187983" target="_blank" rel="noopener">https://blog.csdn.net/qq_41608822/article/details/106187983</a></p>
<p><strong>Ch4.1-Ch4.5</strong>：韦国梁</p>
<p>见正文</p>
<p><strong>Ch4.6-Ch5.2</strong>：崔昊川</p>
<p><a href="https://blog.csdn.net/qq_41608822/article/details/106289904" target="_blank" rel="noopener">https://blog.csdn.net/qq_41608822/article/details/106289904</a> </p>
<p><strong>Ch5.8-Ch6.3</strong>：崔昊川</p>
<p><a href="https://blog.csdn.net/qq_41608822/article/details/106468105" target="_blank" rel="noopener">https://blog.csdn.net/qq_41608822/article/details/106468105</a> </p>
<hr>
<h4 id="2-1-A-k-Armed-Bandit-Problem"><a href="#2-1-A-k-Armed-Bandit-Problem" class="headerlink" title="2.1 A k-Armed Bandit Problem"></a>2.1 A k-Armed Bandit Problem</h4><p>在k-手臂匪徒问题中， 你每次面临k个不同选项或操作中的选择。 在每次选择后，你会收到一个数值奖励，该数值奖励取决于您选择的行动的平稳概率分布。 你的目标是最大化某段时间内的预期总回报，例如，超过1000个动作选择或时间步骤。</p>
<p> 在我们的k-手臂匪徒问题中，考虑到该行动被选中，k个行动中的每一个行动都具有预期的或平均的奖励; 现在让我们将其称之为该行动的价值。 我们表示在时间步骤 t 选择的行动为 At ，相应的回报为 Rt 。 q∗(a)表示为任意行为的值是 a的时候， 选择时的期望回报：</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image002-1591233462658.jpg" alt="img"></p>
<p>如果我们知道每个action对应的value，那么我们只需要每次都选择最高的那个value对应的action即可，但事实却是我们在玩游戏之前，不知道每个action确切的value，我们可以通过多次测试来估计每个action的value,将t 时刻的action a对应的估计价值（estimated value) 记作 Qt(a) ,我们的目标是使得 Qt(a)尽可能的接近q∗(a),然后根据Qt(a)选择 a.</p>
<h4 id="2-2-Action-Value-Methods"><a href="#2-2-Action-Value-Methods" class="headerlink" title="2.2 Action-Value Methods"></a><strong>2.2 Action-Value Methods</strong></h4><p>最简单的value 估计方式就是sample-average（采样平均），即</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image004-1591233462658.jpg" alt="img">其中，</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image006-1591233462658.jpg" alt="img"></p>
<p>简单来说就是对每个action a对应的奖励做平均。通过增加实验次数，当实验次数趋于无穷大时，Qt(a) 趋于 q∗(a)</p>
<p><strong>greedy 与 ϵ-greedy</strong></p>
<p>greedy方法：</p>
<p>如果我们每次都选择最大value对应的action，即</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image008-1591233462658.jpg" alt="img"></p>
<p>那么我们采取的是greedy policy。</p>
<p>ϵ-greedy：</p>
<p>如果我们会在一些时间内选择最大value对应action外的其他action，则表明我们对环境进行了Exploration。ϵ-greedy方法在greedy方法基础上进行改进：</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image010-1591233462658.jpg" alt="img"></p>
<h4 id="2-3-The-10-armed-Testbed"><a href="#2-3-The-10-armed-Testbed" class="headerlink" title="2.3 The 10-armed Testbed"></a><strong>2.3 The 10-armed Testbed</strong></h4><p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image012-1591233462658.jpg" alt="img"></p>
<p>要评测强化学习的优劣，需要一个模型，the 10-arm testbed就是其中一个，可以一次在2000组上进行评测，在每一组中，每一组有一个独立的模型，每一次选择a对应的reward是通过q*(a)的一个高斯分布来返回的，然后再所有组上的表现可以代表我们算法的平均行为。</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image014-1591233462658.jpg" alt="img"></p>
<p>上图简单来说代表了greedy和ϵ-greedy的优劣。通常来说，两种方法都会随着探索次数增加，效果越来越好。Greedy方法可能刚开始比其他更快，但是长远来看，由于短时，不探索的原因，最后效果不如ϵ-greedy。</p>
<p>通常来说，如果任务方差小，greedy表现更好，反之ϵ-greedy表现更好。所以在实际中，两种方法需要在exploration 和exploitation之间找到平衡。</p>
<h4 id="2-4-Incremental-Implementation"><a href="#2-4-Incremental-Implementation" class="headerlink" title="2.4 Incremental Implementation"></a><strong>2.4 Incremental Implementation</strong></h4><p>上一节介绍的sample average方法是将所有的历史信息全部记录下来然后求平均，如果历史记忆很长，甚至无限，那么直接求解将受到内存和运行时间的限制。下面将结合数学中的迭代思想推导出增量式的求解方法，该方法仅耗费固定大小的内存，且可以单步更新。</p>
<p>为了简化表达，我们只关注一个action,假设Ri是第 i 次选择这个action所获得的reward， 将这个action第n 次被选择的estimate value表示为Qn+1，</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image016-1591233462658.jpg" alt="img"></p>
<hr>
<h4 id="3-1-“智能体-环境”交互接口"><a href="#3-1-“智能体-环境”交互接口" class="headerlink" title="3.1 “智能体-环境”交互接口"></a>3.1 “智能体-环境”交互接口</h4><p>有限马尔可夫决策过程MDP是一种通过交互式学习来实现目标的理论框架，包含以下元素：</p>
<p>智体：实施决策的机器</p>
<p>环境: 智体之外所有与其相互作用的事务</p>
<p>收益：智体选择交互后，环境会呈现新的状态，并且产生收益，智体的目标是最大化累计收益。</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image002.jpg" alt="img"></p>
<p>有限马尔可夫性：</p>
<p>St和Rt的每个可能的值出现的概率只取决于前一个状态St-1和前一个动作At-1，并且与更早之前的状态和动作无关。</p>
<p>基于以上特性，可以推出以下四个公式：</p>
<p>随机变量Rt和St的概率分布：</p>
<p>P（s’,r|s,a）=Pr{St=s’,Rt=r|St-1=s,At-1=a}</p>
<p>状态转移概率：</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image004.jpg" alt="img"></p>
<p>状态-动作 二元组的期望收益：</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image006.jpg" alt="img"></p>
<p>状态-动作-后继状态 三元组的期望收益：</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image008.png" alt="img"></p>
<h4 id="3-2-目标和收益"><a href="#3-2-目标和收益" class="headerlink" title="3.2 目标和收益"></a><strong>3.2</strong> <strong>目标和收益</strong></h4><p>在强化学习中，智能体的目标被形式化表征为一种特殊的信号，称为收益，通过环境传递给智能体。</p>
<p>我们所有的目标都可以归结为：最大化智能体接收到的标量信号（收益）累积和的概率期望值。</p>
<p>一般的，我们将目标设定为取得最大收益时的情况，不可以将收益设定为完成目标的先验知识。举例：在国际象棋中，如果设定吃子为收益，而不是达成胜利，智能体可能为了尽可能多的吃子，达成最大收益而输掉比赛。</p>
<h4 id="3-3-回报和分幕"><a href="#3-3-回报和分幕" class="headerlink" title="3.3 回报和分幕"></a><strong>3.3</strong> <strong>回报和分幕</strong></h4><p>在有终止情况的任务下，比如下棋，智能体和环境交互可以被自然的分成一系列子系列，每个包含终止情况的子系列称为幕。</p>
<p>对于分慕情况，优化目标为：</p>
<p>Gt=Rt+1 + Rt+2 + Rt+3 + Rt+4 +…+RT</p>
<p>T为终止时刻</p>
<p>而对于无法分幕的连续过程，优化目标调整为：</p>
<p>Gt=Rt+1 + yRt+2 +y^2Rt+3+…</p>
<p>Gt=Rt+1+yGt+1</p>
<p>其中</p>
<p>0&lt;=y&lt;=1</p>
<p>y称为折扣率，引入折扣率是防止G趋于无穷大。折扣率决定了未来收益的现值：未来时刻k的收益率只有它的当前值的y^（k-1）倍。y越小，智能体越关注当前收益，y越大，智能体相对越有远见，关注未来收益/</p>
<p> 本章节主要介绍了基于动态规划来求解最优策略。</p>
<hr>
<h4 id="4-1策略评估"><a href="#4-1策略评估" class="headerlink" title="4.1策略评估"></a>4.1策略评估</h4><p> 对于策略Π，其状态价值函数定义如下：</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image002-1591233191615.jpg" alt="img"></p>
<p>其中，Π（a|s）代表策略，即状态s情况下，选取行动a的概率，由智能体决定。</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image004-1591233191615.jpg" alt="img">为转移概率，是系统的动态特性。</p>
<p>理论上，知道了系统的动态特性和用户的策略，则基于上述公式，仅有状态价值函数为未知数，而对s个状态，有s个等式，可以直接求解，但是由于计算繁琐，采用迭代法求解。</p>
<p>其中，初始化时，除了终止状态的价值函数为0，其余均可以是随机数，采用以下式子迭代：</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image006-1591233191615.jpg" alt="img"></p>
<p>理论上k趋于无穷，终会收敛，但是我们可以在变化很小的时候就停止计算。计算流程如下：</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image008.jpg" alt="img"></p>
<h4 id="4-2策略改进"><a href="#4-2策略改进" class="headerlink" title="4.2策略改进"></a>4.2策略改进</h4><p>知道了给定策略下的价值函数，则可以对策略进行改进。</p>
<p>考虑一种情况，在s状态下，选择新的策略，其余状态下保持策略Π不变，则此动作的价值函数为，</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image010.jpg" alt="img"> </p>
<p>由于其余状态下的策略都一样，要评判策略Π’是否优于策略Π，关键在于判断</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image012.jpg" alt="img"></p>
<p>如果上式成立，则<img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image014.jpg" alt="img"></p>
<p>证明如下：</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image016.jpg" alt="img"></p>
<p>所以证明了，一个策略，如果在一个状态下的动作优于另一个状态下的动作，其余 状态下动作相同，则此策略优于后者策略。由此我们可以使用一种贪心法来求解最优策略，在每个状态选取价值函数最高的动作，不断更新策略。由于动作空间有限，必然可以找到最优策略。</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image018.jpg" alt="img"></p>
<h4 id="4-3策略迭代"><a href="#4-3策略迭代" class="headerlink" title="4.3策略迭代"></a>4.3策略迭代</h4><p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image020.jpg" alt="img"></p>
<p>在每个策略中，使用策略评估，计算当前策略的价值函数，然后通过策略改进，得到一个更优的策略，在新的策略中，又需要使用策略评估来收敛，依次循环。</p>
<p>由于一个有限MDP必然只有有限种策略，所以在有限次策略迭代中，必然收敛到最有策略。</p>
<p>流程如下图：</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image022.jpg" alt="img"></p>
<h4 id="4-4价值迭代"><a href="#4-4价值迭代" class="headerlink" title="4.4价值迭代"></a>4.4价值迭代</h4><p>策略迭代的一个缺点是，每次迭代都要使用策略评估，策略评估的收敛非常耗时，可不可以不等策略vΠ收敛就进行策略改进呢？一种特例是，每次评估，只对每个状态更新一次，这种更新方式叫价值迭代。公式如下：</p>
<p><img src="/2020/06/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6/clip_image024.jpg" alt="img"></p>
<h4 id="4-5异步动态规划"><a href="#4-5异步动态规划" class="headerlink" title="4.5异步动态规划"></a>4.5异步动态规划</h4><p>之前的DP的一大缺点在于，它涉及到对整个状态集的操作，也就是说，他需要对整个状态集进行遍历，如果状态集合很大，哪怕单次遍历，开销也是无法承受的。</p>
<p>异步DP算法是一种就地迭代的算法，随机选择状态值就行更新，可能一个状态值被更新一次，另外一些已经被更新多次。但只要保证，某个时间点，所有状态值都被更新，它终会收敛。</p>
<p>一种典型的异步更新是，在价值迭代的基础上，每次只更新一个状态sk的值，而不是遍历状态集合。</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
  </entry>
</search>
