<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Q-learning（1）</title>
    <url>/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/</url>
    <content><![CDATA[<h2 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h2><p>李宏毅老师主页18年机器学习教程： <a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html" target="_blank" rel="noopener">http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html</a> （老师的2020年视频也出了）</p>
 <a id="more"></a> 
<h3 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h3><p>值函数分为： State value function ， State-action value function </p>
<p>状态值函数$V_\pi(s)$，是对状态s的估计</p>
<h4 id="如何估计状态值函数"><a href="#如何估计状态值函数" class="headerlink" title="如何估计状态值函数"></a>如何估计状态值函数</h4><p><strong>monte-carlo</strong>方法：</p>
<p>对多个episode求平均，可以用神经网咯拟合</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586575571217.png" alt="1586575571217"></p>
<p><strong>TD</strong>方法：</p>
<p>两个状态之间的值函数之差是reward，拟合reward就可以了</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586575379396.png" alt="1586575379396"></p>
<p>注：MC方法相较来说<strong>方差</strong>更大，并且这两种估计方法可能结果不一样大</p>
<h3 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h3><p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586576088554.png" alt="1586576088554" style="zoom: 33%;"></p>
<p>对于一个状态，选择一个最大的Q func就可以了</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586585731110.png" alt="1586585731110" style="zoom:33%;"></p>
<p>证明：</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586585767328.png" alt="1586585767328" style="zoom:33%;"></p>
<h4 id="三个技巧"><a href="#三个技巧" class="headerlink" title="三个技巧"></a>三个技巧</h4><p><strong>Target network</strong></p>
<p>由于对q func的拟合是用的两个q之差与reward的比较，所以需要固定住一个</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586585935056.png" alt="1586585935056" style="zoom: 33%;"></p>
<p><strong>Exploration</strong></p>
<p>策略更新的方式是选择q(s,a)最大的，这在前面阶段肯定是不合适的，因为很多(s,a)没有被遍历过。</p>
<p>两种解决方法如下图</p>
<p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586586846153.png" alt="1586586846153" style="zoom:33%;"></p>
<p><strong>replay buffer</strong></p>
<p>弄一个缓存，里面放（st,at,rt,st+1）</p>
<p>每次从buffur里取一个batch，然后训练q func</p>
<h4 id="Typical-Q-Learning-Algorithm"><a href="#Typical-Q-Learning-Algorithm" class="headerlink" title="Typical Q-Learning Algorithm"></a>Typical Q-Learning Algorithm</h4><p><img src="/2020/04/09/Q-learning%EF%BC%881%EF%BC%89/1586587733417.png" alt="1586587733417"></p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
  </entry>
  <entry>
    <title>强化学习PPO</title>
    <url>/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/</url>
    <content><![CDATA[<h2 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h2><h3 id="basic"><a href="#basic" class="headerlink" title="basic"></a>basic</h3><p>每一个eposide记录为$\tau$</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/4155986-0da7e5f276ec5aca.webp" style="zoom:80%;"></p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583120692790.png" alt="1583120692790" style="zoom: 33%;"></p>
 <a id="more"></a> 
<p>$p_\theta(\tau)$是按策略参数为$\theta$得到这样一个trajectory的概率</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583120878285.png" alt="1583120878285" style="zoom: 33%;"></p>
<p>要提高这个期望，使用梯度上升，对$\theta$求梯度使用上式中的中间这个求和形式，使用技巧</p>
<script type="math/tex; mode=display">
\bigtriangledown f(x) = f(x)\bigtriangledown \log{f(x)}</script><p>这样可以转化为概率采样的形式，</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\bigtriangledown\overline{R_\theta} & =\bigtriangledown\sum_{\tau}{R(\tau)p_\theta(\tau)}
\\& = \sum_\tau{R(\tau)p_\theta(\tau)\bigtriangledown \log{p_\theta(\tau)}}
\\& = E_{\tau\sim p_\theta}(R(\tau)\bigtriangledown{\log{p_\theta(\tau)}})
\\& \approx \frac{1}{N}\sum_{n=1}^N{R(\tau^n)\bigtriangledown{\log{p_\theta(\tau^n)}}}
\\& = \frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_n}R(\tau^n)\bigtriangledown{\log{p_\theta (a_t^n|s_t^n)}}
\end{aligned}</script><p>最后一行中因为我们的梯度不关心$p(s_{t+1}|s_t,a_t)$所以直接忽略掉，$R(\tau^n)$应该放在积分号中间也可以。</p>
<p>我们可以看到这个梯度与$\theta$有关，所以每次更新梯度，都需要重新采样N次，也就是需要与环境交互，跑一个$\tau$出来，然后更新梯度，然后就需要重新再交互，这相当于是on-policy，换成off-policy就是PPO。</p>
<p>进行ppo之前先有两个tips。</p>
<h4 id="tip-1-add-a-base-line"><a href="#tip-1-add-a-base-line" class="headerlink" title="tip 1: add a base line"></a>tip 1: add a base line</h4><p>PG方法在更新策略时，基本思想就是增加reward大的动作出现的概率，减小reward小的策略出现的概率。假设现在有一种情况，我们的reward在无论何时都是正的，对于没有采样到的动作，它的reward是0。因此，如果一个比较好的动作没有被采样到，而采样到的不好的动作得到了一个比较小的正reward，那么没有被采样到的好动作的出现概率会越来越小，这显然是不合适的，因此我们需要增加一个奖励的基线，让reward有正有负。 增加的基线是所获得奖励的平均值b。</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583136617036.png" alt="1583136617036" style="zoom:33%;"></p>
<h4 id="tip-2-Assign-Suitable-Credit"><a href="#tip-2-Assign-Suitable-Credit" class="headerlink" title="tip 2: Assign Suitable Credit"></a>tip 2: Assign Suitable Credit</h4><p>当前动作之后获得的奖励才重要</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583137118284.png" alt="1583137118284" style="zoom:33%;"></p>
<p>添加一个discount，前面那个系数可以用Advantage Function表示：$A^\theta(s_t,a_t)$</p>
<p>这个相当于在时间t之后得到的<em>价值总和</em>（忘了专业名词叫啥了）</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583137342234.png" alt="1583137342234" style="zoom:33%;"></p>
<h3 id="Proximal-Policy-Optimization"><a href="#Proximal-Policy-Optimization" class="headerlink" title="Proximal Policy Optimization"></a>Proximal Policy Optimization</h3><p>采用重要性采样</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583137476846.png" alt="1583137476846" style="zoom:33%;"></p>
<p>可以从p的分布采样，换到从q的分布采样，需要注意，p和q的分布差异不能太大，因为虽然上式的均值相同，但是方差不同，还是会有影响。</p>
<script type="math/tex; mode=display">
\bigtriangledown\overline{R_\theta}= \frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_n}R(\tau^n)\bigtriangledown{\log{p_\theta (a_t^n|s_t^n)}}</script><p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583151524298.png" alt="1583151524298" style="zoom:33%;"><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583137724390.png" alt="1583137724390" style="zoom:33%;"></p>
<p>这里为什么是从$\pi_\theta$中sample出$(s_t,a_t)$怪不清楚的，现在的理解是</p>
<p>我们需要控制$\theta$和$\theta’$相似，使用KL divergence，这里肯定比较的是$p_\theta(\bullet|s)$和$p_{\theta’}(\bullet|s)$的距离</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583151733566.png" alt="1583151733566" style="zoom:33%;"></p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583151778726.png" alt="1583151778726" style="zoom:33%;"></p>
<p>算法流程：</p>
<ul>
<li>Initial policy parameters $\theta^0$</li>
<li>In each iteration <ul>
<li>Using $\theta^k$ to interact with the environment to collect $s_t,a_t$  and compute advantage $A^{\theta^k}(s_t,a_t)$</li>
<li>Find $\theta$ optimizeing $J^{\theta^k}(\theta)-\beta KL(\theta,\theta^k)$ <img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583152327757.png" alt="1583152327757" style="zoom: 50%;"></li>
</ul>
</li>
</ul>
<p>上面这些 来自于<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2018/Lecture/PPO (v3" target="_blank" rel="noopener">李宏毅课程</a>.pdf) 。</p>
<p>有几点没有懂</p>
<ul>
<li>新的策略需要凑一条条完整的流程$\tau$吗</li>
<li>advantage function 应该怎么计算</li>
<li>为什么要优化函数J</li>
</ul>
<p>更正：</p>
<p>重新看了下课程，之前弄错了，$\theta’$才是与环境交互的:smile:</p>
<p>优化的J的原因是，$\bigtriangledown f(x) = f(x)\bigtriangledown \log{f(x)}$相当于原函数是f(x)，也就是这里的J</p>
<p>在算kl divergence的时候，把所有sample到的$s_t$都算下两个model的输出的区别，这个好像还比较麻烦，具体怎么处理还不清楚</p>
<h4 id="TRPO"><a href="#TRPO" class="headerlink" title="TRPO"></a>TRPO</h4><p>简单写下PPO的前身，更麻烦的处理方式，TRPO讲KL divergence项作为一个constrain，不在optimize项里面。</p>
<h4 id="PPO2"><a href="#PPO2" class="headerlink" title="PPO2"></a>PPO2</h4><p>PPO2 提出了不用计算KL Divergence的方法</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583206637985.png" alt="1583206637985" style="zoom:67%;"></p>
<p>clip的意思是如果$p_\theta$和$p_{\theta’}$ 的比值如果小于$1-\varepsilon$则等于$1-\varepsilon$。</p>
<p>图中横轴是clip里的第一项纵轴是输出结果。</p>
<p>具体算法流程：</p>
<p><img src="/2020/04/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/1583284441674.png" alt="1583284441674"></p>
<p>可以看到在训练过程中，是需要再学一个value function去估测advantage的。</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
  </entry>
  <entry>
    <title>强化基础复习</title>
    <url>/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/</url>
    <content><![CDATA[<p>看李宏毅老师的深度强化会感觉比较东西比较偏应用，正好要看多智体相关的东西，找了张伟楠老师的主页 <a href="http://wnzhang.net/" target="_blank" rel="noopener">http://wnzhang.net/</a> ，顺便复习下</p>
<a id="more"></a>
<h2 id="bellman-equation-for-value-function"><a href="#bellman-equation-for-value-function" class="headerlink" title="bellman equation for value function"></a>bellman equation for value function</h2><p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586868221816.png" alt="1586868221816"></p>
<p>$P_{s\pi}$ is state transition. At state s, use policy pi</p>
<p>Optimal value function:</p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586868369635.png" alt="1586868369635"></p>
<h2 id="值函数迭代-amp-策略迭代"><a href="#值函数迭代-amp-策略迭代" class="headerlink" title="值函数迭代&amp;策略迭代"></a>值函数迭代&amp;策略迭代</h2><p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586917042173.png" alt="1586917042173"></p>
<p>需要已知转移矩阵和reward function</p>
<p>这里的reward在这里应该是假设只依赖于状态s</p>
<p>动态规划：</p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586941139436.png" alt="1586941139436" style="zoom:33%;"></p>
<h2 id="model-free"><a href="#model-free" class="headerlink" title="model free"></a>model free</h2><h3 id="model-free-prediction"><a href="#model-free-prediction" class="headerlink" title="model-free prediction"></a>model-free prediction</h3><p><strong>MC</strong></p>
<script type="math/tex; mode=display">
G_t=R_{t+1}+\gamma R_{t+2}+...+\gamma^{T-1}R_{T}</script><script type="math/tex; mode=display">
V^{\pi}(s)=E[G_t|s_t=s,\pi]\approx \frac{1}{N}\sum_{i=1}^NG^{(i)}_{t}</script><p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586919493957.png" alt="1586919493957" style="zoom:33%;"></p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586940567481.png" alt="1586940567481" style="zoom:33%;"></p>
<p><strong>TD</strong></p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586919744733.png" alt="1586919744733" style="zoom:33%;"></p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586940746277.png" alt="1586940746277" style="zoom:33%;"></p>
<h3 id="model-free-control"><a href="#model-free-control" class="headerlink" title="model-free control"></a>model-free control</h3><p><strong>$\epsilon$-greedy</strong></p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1587038476938.png" alt="1587038476938" style="zoom:33%;"></p>
<p>前面的mc和td的目的是估计出值函数，<strong>control是为了得到最优的policy</strong></p>
<p><strong>mc control</strong></p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1586954094046.png" alt="1586954094046" style="zoom:33%;"></p>
<p><strong>SARSA</strong></p>
<p>state-action-reward-state-action  </p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1587038173717.png" alt="1587038173717" style="zoom:33%;"></p>
<p>可以看到，这里是一个q的预测并不是control，但是q func得到之后就很容易得到pi，因为对于同一个s，选择最大的q(s,a)就可以了</p>
<p>在线学习版本：</p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1587039060349.png" alt="1587039060349"></p>
<p>似乎sarsa没有看到有用off-policy的，离线的可能就是q-learning了</p>
<p><strong>Q-learning</strong></p>
<p>可以参看之前的Q-learning（1）里面的q-learning，与本文的表示方式稍微有点不一样</p>
<p><img src="/2020/04/14/%E5%BC%BA%E5%8C%96%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/1587040735589.png" alt="1587040735589" style="zoom:33%;"></p>
<p>这里的$\mu$是 交互的策略</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
  </entry>
  <entry>
    <title>fairness-gym搭建</title>
    <url>/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p> github网址<a href="https://github.com/google/ml-fairness-gym" target="_blank" rel="noopener">https://github.com/google/ml-fairness-gym</a> </p>
<a id="more"></a>
<p>一个仿真环境</p>
<p><a href="https://github.com/google/ml-fairness-gym/blob/master/papers/acm_fat_2020_fairness_is_not_static.pdf" target="_blank" rel="noopener">Fairness is not Static: Deeper Understanding of Long Term Fairness via Simulation Studies</a></p>
<ul>
<li>College admissions</li>
<li>Lending</li>
<li>Attention allocation</li>
</ul>
<p><a href="https://github.com/google/ml-fairness-gym/blob/master/papers/fairmlforhealth2019_fair_treatment_allocations_in_social_networks.pdf" target="_blank" rel="noopener">Fair treatment allocations in social networks</a></p>
<ul>
<li>Infectious disease</li>
</ul>
<h2 id="场景一：-LENDING"><a href="#场景一：-LENDING" class="headerlink" title="场景一：  LENDING"></a>场景一：  LENDING</h2><p>(这个场景相关的设定有三种不同的，gym中文档中最重要，另外两篇论文里的辅助理解)</p>
<p><strong>Delayed Impact of Fair Machine Learning</strong>这篇文章是one-step的，gym提供的是 many steps  </p>
<p>个人被标记一个信用分数$\mathcal{X}:=\{1,2,…,C\}$，分数越高表示越有可能带来正向收益<del>，我的理解是越有可能还上款</del>。</p>
<p>银行采用一个策略$\tau:\mathcal{X}\rightarrow[0,1]$表示选择的概率，我的理解是策略是对一个固定的分数，借钱的概率是相同的。</p>
<p><img src="/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/1587195490449.png" alt="1587195490449" style="zoom: 50%;"></p>
<p>考虑两个群体，A、B，一个优势，一个劣势，分数分布为$\pi_A$，$\pi_B$</p>
<p>A群体占总体人数的$g_A$</p>
<p><img src="/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/1587212231669.png" alt="1587212231669" style="zoom:50%;"></p>
<p><del>没看懂这是个啥</del></p>
<p>例子中写了，如下</p>
<p><img src="/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/1587214193670.png" alt="1587214193670" style="zoom:50%;"></p>
<p>我的理解是这是赚多少钱</p>
<p><img src="/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/1587212998930.png" alt="1587212998930" style="zoom:50%;"></p>
<p><del>持续看不懂成功是指的成功借到钱？</del>看后面的例子说是一个人在固定时间内还钱的概率</p>
<p><img src="/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/1587214352552.png" alt="1587214352552" style="zoom:50%;"></p>
<p>还钱后分数增加，不换分数减少</p>
<hr>
<p><strong>Fairness is not static</strong>中的说明</p>
<p>C表示分数</p>
<p>$\pi(C)$表示成功还款率</p>
<p>$p_{0}^{A}(C)$为A群体的C的分布</p>
<hr>
<p><strong>文档中的说明</strong></p>
<p> <a href="https://github.com/google/ml-fairness-gym/blob/master/docs/quickstart.md" target="_blank" rel="noopener">https://github.com/google/ml-fairness-gym/blob/master/docs/quickstart.md</a> </p>
<p><img src="/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/1587296941877.png" alt="1587296941877"></p>
<p>文档里这段之后给出了一些衡量的说明，但是看了也不知道怎么写自己的策略，现在能想到的解决办法是学习下python的debug，一步一步看下实例代码跑了哪些</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python experiments&#x2F;lending_demo.py --num_steps&#x3D;5000</span><br></pre></td></tr></table></figure>
<p> The classifier is trying to find one or more appropriate threshold(s) for giving out loans. It will follow a simple rule. For the first 200 steps, it will give out loans to everyone. After that, it will use that data to find the threshold with the <strong>highest accuracy</strong>. (In this simulation, the cost of a false positive is exactly equal to the gain from a true positive, so accuracy is a reasonable metric to optimize). </p>
<p>手动分割，看了下<strong>lending_demo.py</strong>代码似乎还是比较易读</p>
<p><code>__future__</code>包和absl之前没用过，可能稍微学一下，中文的介绍比较少 <a href="https://abseil.io/docs/" target="_blank" rel="noopener">https://abseil.io/docs/</a> </p>
<p>下面先记录下重要的代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># experiments/lending_demo.py</span></span><br><span class="line">result = lending.Experiment(</span><br><span class="line">    <span class="comment"># 0.5</span></span><br><span class="line">    group_0_prob=group_0_prob,</span><br><span class="line">    <span class="comment"># 利率</span></span><br><span class="line">    interest_rate=<span class="number">1.0</span>,</span><br><span class="line">    bank_starting_cash=<span class="number">10000</span>,</span><br><span class="line">    seed=<span class="number">200</span>,</span><br><span class="line">    num_steps=FLAGS.num_steps,</span><br><span class="line">    <span class="comment"># 我猜是For the first 200 steps, it will give out loans to everyone.</span></span><br><span class="line">    burnin=<span class="number">200</span>,</span><br><span class="line">    <span class="comment"># 暂时不知道是啥</span></span><br><span class="line">    cluster_shift_increment=<span class="number">0.01</span>,</span><br><span class="line">    include_cumulative_loans=<span class="literal">True</span>,</span><br><span class="line">    return_json=<span class="literal">False</span>,</span><br><span class="line">    <span class="comment"># policy就是在这个写</span></span><br><span class="line">    <span class="comment"># MAXIMIZE_REWARD = threshold_policies.ThresholdPolicy.MAXIMIZE_REWARD</span></span><br><span class="line">	<span class="comment"># EQUALIZE_OPPORTUNITY = threshold_policies.ThresholdPolicy.EQUALIZE_OPPORTUNITY</span></span><br><span class="line">    threshold_policy=(EQUALIZE_OPPORTUNITY <span class="keyword">if</span> FLAGS.equalize_opportunity <span class="keyword">else</span></span><br><span class="line">                      MAXIMIZE_REWARD)).run()</span><br></pre></td></tr></table></figure>
<p>先看看最简单的<code>MAXIMIZE_REWARD</code>是怎么写的</p>
<p>在agents/threshold_policies.py中看到MAXIMIZE_REWARD只是个枚举类</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ThresholdPolicy</span><span class="params">(enum.Enum)</span>:</span></span><br><span class="line">  SINGLE_THRESHOLD = <span class="string">"single_threshold"</span></span><br><span class="line">  MAXIMIZE_REWARD = <span class="string">"maximize_reward"</span></span><br><span class="line">  EQUALIZE_OPPORTUNITY = <span class="string">"equalize_opportunity"</span></span><br></pre></td></tr></table></figure>
<p>lending中的Experiment对象所有的变量都是<code>attr.ib()</code>，暂时不影响理解</p>
<p>Experiment对象首先给了三种参数，env的参数，agent的参数，run的参数</p>
<p>然后定义了两个函数，<code>scenario_builder</code>和<code>run</code></p>
<p><code>run</code>里面调用<code>scenario_builder</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">env = lending.DelayedImpactEnv(env_params)</span><br></pre></td></tr></table></figure>
<p>这里调用的lending是environment中的</p>
<p><img src="/2020/04/18/fairness-gym%E6%90%AD%E5%BB%BA/1587440760162.png" alt="1587440760162" style="zoom:50%;"></p>
<p>这里定义的lending的环境有这几种</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">agent = oracle_lending_agent.OracleThresholdAgent(</span><br><span class="line">            action_space=env.action_space,</span><br><span class="line">            reward_fn=rewards.BinarizedScalarDeltaReward(</span><br><span class="line">                <span class="string">'bank_cash'</span>, baseline=env.initial_params.bank_starting_cash),</span><br><span class="line">            observation_space=env.observation_space,</span><br><span class="line">            params=agent_params,</span><br><span class="line">            env=env)</span><br></pre></td></tr></table></figure>
<p>Threshold agent with oracle access to distributional data.</p>
<p>OracleThresholdAgent的父类是classifier_agents.ThresholdAgent，他的父类是ScoringAgent，他的父类是core.Agent</p>
<p>agent这里似乎是一些参数的定义，具体的agent的算法似乎是没有看到，reward的算法定义了，reward的func就几行暂时不看，下面看run</p>
<p>看了下重要的代码应该是这个</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">metric_results = run_util.run_simulation(env, agent, metrics,                                         self.num_steps, self.seed)</span><br></pre></td></tr></table></figure>
<p>然后找到</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">action = agent.act(observation, done)</span><br></pre></td></tr></table></figure>
<p>用pycharm debug到这个agent的base class是ScoringAgent</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
  </entry>
  <entry>
    <title>python面向对象</title>
    <url>/2020/04/21/python%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/</url>
    <content><![CDATA[<p>看fair gym然后发现我对python里的继承的用法有些误解</p>
<a id="more"></a>
<p>使用教程 <a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017495723838528" target="_blank" rel="noopener">https://www.liaoxuefeng.com/wiki/1016959663602400/1017495723838528</a> </p>
<p>想要学到知识</p>
<ul>
<li>[x] 继承是继承哪些东西，方法和属性？</li>
<li>[x] 有没有private类</li>
<li>[ ] 怎么选择继承</li>
</ul>
<h2 id="class"><a href="#class" class="headerlink" title="class"></a>class</h2><p>所有类最终的父类都是object</p>
<p> 如果要让内部属性不被外部访问，可以把属性的名称前加上两个下划线<code>__</code>，在Python中，实例的变量名如果以<code>__</code>开头，就变成了一个私有变量（<strong>private</strong>），只有内部可以访问，外部不能访问 </p>
<p>看到 <a href="https://blog.csdn.net/brucewong0516/article/details/79119551" target="_blank" rel="noopener">https://blog.csdn.net/brucewong0516/article/details/79119551</a> 中提到<strong>实例方法、静态方法和类方法</strong> </p>
<p><strong>实例方法</strong></p>
<p>第一个参数self，可以通过self访问类属性，这个好像我自己常用的方法</p>
<p><strong>静态方法</strong></p>
<p>定义的时候使用<code>@staticmethod</code>装饰器，打开了新世界，。。。不需要实例参数也不需要类参数</p>
<p>看起来很迷幻。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">person</span><span class="params">(object)</span>:</span></span><br><span class="line">    tall = <span class="number">180</span></span><br><span class="line">    hobbies = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age,weight)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.weight = weight</span><br><span class="line"><span class="meta">    @staticmethod    #静态方法装饰器</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">infoma</span><span class="params">()</span>:</span>     <span class="comment">#没有参数限制，既不要实例参数，也不用类参数</span></span><br><span class="line">        print(person.tall)</span><br><span class="line">        print(person.hobbies)</span><br><span class="line"><span class="comment">#person.infoma()   #静态法可以通过类名访问</span></span><br><span class="line">Bruce = person(<span class="string">"Bruce"</span>, <span class="number">25</span>,<span class="number">180</span>)   <span class="comment">#通过实例访问</span></span><br><span class="line">Bruce.infoma()</span><br></pre></td></tr></table></figure>
<p><strong>类方法</strong></p>
<p>类方法<strong>以cls作为第一个参数</strong>，<strong>cls表示类本身</strong>，定义时<strong>使用@classmethod装饰器</strong>。通过cls可以访问类的相关属性。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">person</span><span class="params">(object)</span>:</span></span><br><span class="line">    tall = <span class="number">180</span></span><br><span class="line">    hobbies = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age,weight)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.weight = weight</span><br><span class="line"><span class="meta">    @classmethod     #类的装饰器</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">infoma</span><span class="params">(cls)</span>:</span>   <span class="comment">#cls表示类本身，使用类参数cls</span></span><br><span class="line">        print(cls.__name__)</span><br><span class="line">        print(dir(cls))</span><br><span class="line"><span class="comment">#cls表示类本身</span></span><br><span class="line"><span class="comment">#person.infoma()  直接调用类的装饰器函数，通过cls可以访问类的相关属性</span></span><br><span class="line">Bruce = person(<span class="string">"Bruce"</span>, <span class="number">25</span>,<span class="number">180</span>)   <span class="comment">#也可以通过两步骤来实现，第一步实例化person，第二步调用装饰器</span></span><br><span class="line">Bruce.infoma()</span><br></pre></td></tr></table></figure>
<p>就像学了假python</p>
<h2 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h2><p>继承很简单，在定义子类的时候把父类放在括号里就可以</p>
<p>父类的方法和属性都会被继承</p>
<p>子类自定义属性、方法可以覆盖父类，实现多态</p>
<p>这个时候如果要访问父类的属性，可以使用直接通过父类名调用，需要把self传进去不过（有点神奇，这个self应该是子类，相当于把子类传到了父类？）。</p>
<p>但是这样需要父类名硬编码到子类（忘了啥是硬编码了），所以推荐使用<code>super</code></p>
<p><code>super(Child,self)</code>，第一个参数子类，第二个参数父类</p>
<h2 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h2><p>这个东西很有趣，顺便学一下， <a href="https://foofish.net/python-decorator.html" target="_blank" rel="noopener">https://foofish.net/python-decorator.html</a> </p>
<p>装饰器本质上是一个 Python 函数或类，它可以让其他函数或类在不需要做任何代码修改的前提下增加额外功能，装饰器的返回值也是一个函数/类对象。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_logging</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">()</span>:</span></span><br><span class="line">        logging.warn(<span class="string">"%s is running"</span> % func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func()   <span class="comment"># 把 foo 当做参数传递进来时，执行func()就相当于执行foo()</span></span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'i am foo'</span>)</span><br><span class="line">foo = use_logging(foo)  <span class="comment"># 因为装饰器 use_logging(foo) 返回的时函数对象 wrapper，这条语句相当于  foo = wrapper</span></span><br><span class="line">foo()                   <span class="comment"># 执行foo()就相当于执行 wrapper()</span></span><br></pre></td></tr></table></figure>
<p>有点意思，层层套娃</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_logging</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">()</span>:</span></span><br><span class="line">        logging.warn(<span class="string">"%s is running"</span> % func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func()</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"><span class="meta">@use_logging</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"i am foo"</span>)</span><br><span class="line">foo()</span><br></pre></td></tr></table></figure>
<p>这里这个@就替代了上面的foo=use_logging(foo)</p>
<p>注意这里的foo()是没有参数的</p>
<p>如果有多个参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args)</span>:</span></span><br><span class="line">        logging.warn(<span class="string">"%s is running"</span> % func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure>
<p>  foo 函数还定义了一些关键字参数呢？ </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        <span class="comment"># args是一个数组，kwargs一个字典</span></span><br><span class="line">        logging.warn(<span class="string">"%s is running"</span> % func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure>
<p>奇怪的知识又增加了</p>
<p>装饰器还可以带参数（暂时不记。</p>
<hr>
<p>分割，上面讲的是函数的装饰器</p>
<p>还有<strong>类装饰器</strong></p>
<p> 使用类装饰器主要依靠类的<code>__call__</code>方法，当使用 @ 形式将装饰器附加到函数上时，就会调用此方法。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, func)</span>:</span></span><br><span class="line">        self._func = func</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'class decorator runing'</span>)</span><br><span class="line">        self._func()</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'class decorator ending'</span>)</span><br><span class="line"><span class="meta">@Foo</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'bar'</span>)</span><br><span class="line">bar()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
</search>
